# üöÄ API Gemini - Structure des requ√™tes JSON

## üìã **Vue d'ensemble**

Ce document d√©crit la structure JSON des requ√™tes envoy√©es √† l'API Google Gemini via le SDK `@google/genai` pour les fonctionnalit√©s de chat de l'application.

## üîó **Endpoint et Configuration**

- **SDK utilis√©** : `@google/genai` (remplacement de `@google/generative-ai`)
- **Mod√®le par d√©faut** : `gemini-2.0-flash-001`
- **M√©thode** : `genAI.models.generateContent()`

## üì® **Structure de la requ√™te JSON**

### **Requ√™te Chat Standard**

```json
{
  "model": "gemini-2.0-flash-001",
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "Premi√®re question de l'utilisateur"
        }
      ]
    },
    {
      "role": "model",
      "parts": [
        {
          "text": "R√©ponse de l'IA"
        }
      ]
    },
    {
      "role": "user",
      "parts": [
        {
          "text": "Message actuel de l'utilisateur"
        }
      ]
    }
  ],
  "config": {
    "generationConfig": {
      "temperature": 0.7,
      "maxOutputTokens": 2048
    }
  }
}
```

### **Requ√™te Chat avec Contexte Int√©gr√©**

```json
{
  "model": "gemini-2.0-flash-001",
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "CONTEXTE IMPORTANT √Ä PRENDRE EN COMPTE POUR TOUTES MES R√âPONSES:\n\n[Contenu du ContextView]\n\n---\n\nTu dois utiliser ce contexte pour r√©pondre de mani√®re plus pr√©cise et pertinente √† mes questions. N'h√©site pas √† faire r√©f√©rence aux √©l√©ments du contexte quand c'est appropri√©."
        }
      ]
    },
    {
      "role": "model",
      "parts": [
        {
          "text": "J'ai bien pris en compte le contexte fourni. Je l'utiliserai pour r√©pondre de mani√®re plus pr√©cise et pertinente √† vos questions."
        }
      ]
    },
    {
      "role": "user",
      "parts": [
        {
          "text": "Premi√®re question de l'utilisateur"
        }
      ]
    },
    {
      "role": "model",
      "parts": [
        {
          "text": "R√©ponse de l'IA utilisant le contexte"
        }
      ]
    },
    {
      "role": "user",
      "parts": [
        {
          "text": "Message actuel de l'utilisateur"
        }
      ]
    }
  ],
  "config": {
    "generationConfig": {
      "temperature": 0.7,
      "maxOutputTokens": 2048
    }
  }
}
```

## üéØ **Int√©gration du contexte dans le chat**

### **Principe de fonctionnement**

Le contexte du `ContextView` est automatiquement int√©gr√© √† **chaque** conversation de chat pour que l'IA prenne en compte ces informations dans toutes ses r√©ponses.

### **Flux d'int√©gration**

1. **R√©cup√©ration** : Le contexte est lu depuis `contextStore.context`
2. **Injection** : Ajout√© au d√©but de la conversation comme message syst√®me
3. **Persistance** : L'IA "se souvient" du contexte pour toute la session
4. **R√©f√©rencement** : L'IA peut faire r√©f√©rence au contexte dans ses r√©ponses

### **Structure de l'injection**

```javascript
// 1. Message syst√®me avec le contexte (si contexte non vide)
{
  role: 'user',
  parts: [{
    text: `CONTEXTE IMPORTANT √Ä PRENDRE EN COMPTE POUR TOUTES MES R√âPONSES:

${context.trim()}

---

Tu dois utiliser ce contexte pour r√©pondre de mani√®re plus pr√©cise et pertinente √† mes questions. N'h√©site pas √† faire r√©f√©rence aux √©l√©ments du contexte quand c'est appropri√©.`
  }]
}

// 2. R√©ponse de confirmation de l'IA
{
  role: 'model',
  parts: [{
    text: "J'ai bien pris en compte le contexte fourni. Je l'utiliserai pour r√©pondre de mani√®re plus pr√©cise et pertinente √† vos questions."
  }]
}

// 3. Historique normal de la conversation
// 4. Nouveau message utilisateur
```

### **Avantages de cette approche**

- ‚úÖ **Contexte permanent** : L'IA garde le contexte en m√©moire
- ‚úÖ **R√©ponses enrichies** : Utilisation intelligente du contexte
- ‚úÖ **Transparence** : L'utilisateur sait que le contexte est pris en compte
- ‚úÖ **Flexibilit√©** : Contexte modifiable √† tout moment

### **Cas d'usage**

- **Documentation projet** : Ajouter les sp√©cifications techniques
- **Code source** : Inclure des extraits de code √† analyser
- **Instructions sp√©ciales** : D√©finir le style de r√©ponse souhait√©
- **Donn√©es m√©tier** : Fournir des informations sur le domaine

## üîß **Param√®tres de configuration**

### **GenerationConfig**

| Param√®tre | Type | Valeur par d√©faut | Description |
|-----------|------|-------------------|-------------|
| `temperature` | number | 0.7 | Contr√¥le la cr√©ativit√© (0.0 = d√©terministe, 2.0 = tr√®s cr√©atif) |
| `maxOutputTokens` | number | 2048 | Limite maximale de tokens dans la r√©ponse |

### **Mod√®les disponibles**

- `gemini-2.0-flash-001` (par d√©faut)
- `gemini-pro`
- `gemini-pro-vision` (pour images)

## üìä **Types de contenus**

### **R√¥les disponibles**

- `user` : Messages de l'utilisateur
- `model` : R√©ponses de l'IA

### **Structure des parts**

```json
{
  "parts": [
    {
      "text": "Texte du message"
    }
    // Support futur pour images, documents, etc.
  ]
}
```

## üîÑ **Flux de traitement**

### **1. Chat Standard avec Contexte (`processChatMessage`)**

```javascript
// Construction de la requ√™te avec contexte int√©gr√©
const contents = []

// Ajouter le contexte si fourni
if (context && context.trim() !== '') {
  // Message syst√®me avec contexte
  contents.push({
    role: 'user',
    parts: [{ text: `CONTEXTE IMPORTANT √Ä PRENDRE EN COMPTE...` }]
  })
  
  // Confirmation de l'IA
  contents.push({
    role: 'model', 
    parts: [{ text: "J'ai bien pris en compte le contexte..." }]
  })
}

// Ajouter l'historique existant
history.forEach(msg => {
  contents.push({
    role: msg.role,
    parts: [{ text: msg.parts[0].text }]
  })
})

// Ajouter le nouveau message
contents.push({
  role: 'user',
  parts: [{ text: message }]
})
```

### **2. Context Prompt (`processContextPrompt`)**

```javascript
// Construction du prompt enrichi
const fullPrompt = `Tu es un assistant IA sp√©cialis√©...

Contexte fourni:
${currentContext}

Prompt/Instruction:
${prompt}`

// Envoi simple
const contents = [{
  role: 'user',
  parts: [{ text: fullPrompt }]
}]
```

## üì§ **Exemple de r√©ponse Gemini**

```json
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "R√©ponse g√©n√©r√©e par Gemini..."
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0,
      "safetyRatings": [...]
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 156,
    "candidatesTokenCount": 342,
    "totalTokenCount": 498
  }
}
```

## ‚ö†Ô∏è **Gestion des erreurs**

### **Types d'erreurs courantes**

```json
// Erreur d'authentification (401)
{
  "error": {
    "code": 401,
    "message": "Request had invalid authentication credentials."
  }
}

// Erreur de quota (429)
{
  "error": {
    "code": 429,
    "message": "Quota exceeded"
  }
}

// Erreur de contenu (400)
{
  "error": {
    "code": 400,
    "message": "Invalid request content"
  }
}
```

## üîí **S√©curit√© et authentification**

- **API Key** : Pass√©e dans l'en-t√™te `Authorization: Bearer [TOKEN]`
- **Validation** : Token v√©rifi√© via `testConnection()`
- **Stockage** : Token stock√© en localStorage (chiffr√© recommand√©)

## üìù **Notes techniques**

1. **Historique de conversation** : Maintenu c√¥t√© client et envoy√© √† chaque requ√™te
2. **Limite de contexte** : Respecter les limites de tokens du mod√®le
3. **Streaming** : Support via `generateContentStream()` pour r√©ponses en temps r√©el
4. **Retry** : Gestion automatique des tentatives de reconnexion

## üîó **Fichiers source**

- **Service API** : [`src/services/api.js`](src/services/api.js)
- **Store AI** : [`src/stores/ai.js`](src/stores/ai.js)
- **Chat Interface** : [`src/components/ChatView.vue`](src/components/ChatView.vue)
- **Context Interface** : [`src/components/ContextView.vue`](src/components/ContextView.vue)
